#!/usr/bin/python3
# -*- coding: UTF-8 -*-

import json
import os
import requests
import argparse
import xml.etree.ElementTree as ET
from oauth2client.service_account import ServiceAccountCredentials
from googleapiclient.discovery import build

# ä»å‘½ä»¤è¡Œå‚æ•°æå–sitemap


def get_sitemap_path():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--sitemap', help='Path to current file', required=True)
    parser.add_argument(
        '--prevsitemap', help='Path to prev file', required=True)
    args = parser.parse_args()
    global sitemap_path
    global prev_sitemap_path
    sitemap_path = getattr(args, 'sitemap')
    prev_sitemap_path = getattr(args, 'prevsitemap')
    print(f"å½“å‰ï¼š{sitemap_path} \nä¸Šæ¬¡: {prev_sitemap_path}")

# ä»sitemapæå–url


def extract_urls_from_sitemap(sitemap_path):
    urls = []
    with open(sitemap_path, 'r') as f:
        tree = ET.parse(f)
        root = tree.getroot()
        for url in root.findall('{http://www.sitemaps.org/schemas/sitemap/0.9}url'):
            # è·å– loc å…ƒç´ çš„æ–‡æœ¬å†…å®¹
            loc = url.find(
                '{http://www.sitemaps.org/schemas/sitemap/0.9}loc').text
            urls.append(loc)
    return urls

# è·å–æ›´æ–°çš„url


def diff_urls(urls, prev_urls):
    final_urls = list(set(urls) - set(prev_urls))
    return final_urls

# æ¨é€URLåˆ—è¡¨åˆ°ç™¾åº¦ç«™é•¿


def push_urls_to_baidu(urls, site_url):
    token = os.environ["BAIDU_KEY"]  # å› ä¸º$å‰é¢æ˜¯å¤§å†™æ‰€ä»¥ä¹Ÿæ˜¯å¤§å†™
    url = 'http://data.zz.baidu.com/urls?site={}&token={}'.format(
        site_url, token)
    headers = {
        'Content-Type': 'text/plain',
        'User-Agent': 'curl/7.12.1',
        'Host': 'data.zz.baidu.com'
    }
    data = '\n'.join(urls)
    response = requests.post(url, headers=headers, data=data)
    if response.status_code == 200:
        print(f"ğŸ‰ç™¾åº¦æ¨é€æˆåŠŸ!:\n{response.content}")
    else:
        print(f"ğŸ›ç™¾åº¦Error:\n{response.content} ")

# è°ƒç”¨Bing APIæäº¤URL


def push_urls_to_bing(urls, site_url):
    bing_api_key = os.environ["BING_KEY"]
    bing_api_url = "https://ssl.bing.com/webmaster/api.svc/json/SubmitUrlBatch?apikey=" + bing_api_key

    headers = {"Content-Type": "application/json"}

    data = {
        "siteUrl": site_url,
        "urlList": urls
    }

    response = requests.post(bing_api_url, json=data, headers=headers)
    if response.status_code == 200:
        print(f"ğŸ‰Bingæ¨é€æˆåŠŸ!:\n{response.content}")
    else:
        print(f"ğŸ›Bing Error:\n{response.content} ")

# Google index API


def push_urls_to_google(urls):
    SCOPES = ["https://www.googleapis.com/auth/indexing"]

    # service_account_file.json is the private key that you created for your service account.
    JSON_KEY_FILE = json.loads(os.environ["GOOGLE_JSON"])

    credentials = ServiceAccountCredentials.from_json_keyfile_dict(
        JSON_KEY_FILE, scopes=SCOPES)

    # Build service
    service = build('indexing', 'v3', credentials=credentials)
    batch = service.new_batch_http_request(callback=insert_event)

    for url in urls:

        batch.add(service.urlNotifications().publish(
            body={"url": url, "type": 'URL_UPDATED'}))
    batch.execute()

# gooleæ‰¹å¤„ç†å›è°ƒ


def insert_event(request_id, response, exception):
    if exception is not None:
        print(f"ğŸ›Google Exception:\n{exception} ")
    else:
        print(f"ğŸ‰Googleæ¨é€æˆåŠŸ!:\n{response}")


# ä¸»ç¨‹åº
if __name__ == '__main__':
    site_url = 'https://oragekk.me'
    sitemap_path = ''
    prev_sitemap_path = ''
    get_sitemap_path()
    urls = extract_urls_from_sitemap(sitemap_path)
    prev_urls = extract_urls_from_sitemap(prev_sitemap_path)
    final_urls = diff_urls(urls, prev_urls)
    print(f"éœ€è¦æ›´æ–°çš„url:{final_urls}")
    if len(final_urls) > 0:
        push_urls_to_bing(urls, site_url)
        push_urls_to_baidu(urls, site_url)
        push_urls_to_google(final_urls)
    else:
        print("âœ¨æœªå‘ç°è¦æ›´æ–°çš„url")
